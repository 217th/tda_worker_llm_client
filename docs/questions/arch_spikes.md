# Architectural spike backlog — `worker_llm_client`

- Service name: `worker_llm_client`
- Date (local): 2025-12-27 20:55 MSK
- Repo root: `/home/dd-user/projects/tda_llm_client`
- Resolved spikes since generation:
  - `SPK-001` (see `docs/questions/open_questions.md` decision `#50`, 2025-12-27)
  - `SPK-017` (see `docs/questions/open_questions.md` decision `#51`, 2025-12-29)
  - `SPK-018` (see `docs/questions/open_questions.md` decision `#52`, 2025-12-29)
  - `SPK-013` (see `docs/spec/observability.md` “Logger safety gates (MVP)”, 2025-12-29)
  - `SPK-011` (see `docs/spec/implementation_contract.md` “Required subset (MVP)”, 2025-12-29)
  - `SPK-019` (see `docs/spec/error_and_retry_model.md` “Mapping rules (run vs step)”, 2025-12-29)
  - `SPK-003` (see `docs/spec/implementation_contract.md` “Firestore step claim/finalize”, 2025-12-29)
  - `SPK-005` (see `docs/spec/implementation_contract.md` “Self-trigger loops”, 2025-12-29)
- Scope analyzed (no allowlist):
  - `docs/README.md`
  - `docs/spec/**`
  - `docs/contracts/**`
  - `docs/static_model.md`
  - `docs/fixtures/**`
  - `docs/test_vectors/**`
- Read-ignored (user-defined):
  - `docs/checklists/**`
  - `docs/inbox/**`
  - `docs/plan_wbs.md`

| ID | Pri | Category | Decision Question | Why it matters | Evidence (file#section) | Unknowns to resolve | Options (A/B) | Recommended spike (type/timebox) | Validation steps | Done criteria | Dependencies | Confidence |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| SPK-001 | P0 | security_privacy_compliance | How is `GEMINI_API_KEY` stored and injected into Cloud Functions gen2 without ever appearing in logs/artifacts? | Key leakage is a critical security incident; misconfig blocks prod. | `docs/spec/deploy_and_envs.md#Secrets`<br>`docs/spec/observability.md#Security and privacy (minimum)` | Who can read secrets; redaction guarantees; local dev path. | A: Secret Manager → env var injection (simple)<br>B: Fetch from Secret Manager at runtime (less env exposure; more complexity/latency) | Design / 4h | Review CF gen2 Secret Manager wiring<br>Prove logs never include secret (grep-based check on structured logs) | Secret handling documented; explicit “never log” checks in logger/config | SPK-018 | Med |
| SPK-002 | P0 | idempotency_concurrency | What exact GCS write semantics guarantee “exactly-once artifact” with deterministic names (including split-brain recovery)? | Prevents duplicate artifacts and enables safe retries after partial failures. | `docs/spec/implementation_contract.md#Artifact naming (decision)`<br>`docs/spec/error_and_retry_model.md#Idempotency model` | Use of `ifGenerationMatch=0`; handling “object already exists”; atomicity of GCS vs Firestore patches. | A: Create-only upload (`ifGenerationMatch=0`) + treat AlreadyExists as success<br>B: Upload overwrite + store attempt/version (breaks determinism; complicates idempotency) | Prototype / 1d | Implement minimal GCS adapter behavior (create-only + exists-detect)<br>Simulate split-brain path (GCS ok, Firestore fail) using unit tests | Documented semantics; recovery path demonstrated; no second LLM call on retry when artifact exists | SPK-003 | Med |
| SPK-003 | P0 | idempotency_concurrency | How will Firestore `READY → RUNNING` claiming be implemented (preconditions + patch shape) to ensure only one claimant? | Races are expected under at-least-once triggers; incorrect claim causes double LLM calls + inconsistent state. | `docs/spec/architecture_overview.md#High-level flows`<br>`docs/spec/implementation_contract.md#Main scenarios (happy paths)` | Which Firestore API supports `update_time`/`last_update_time` preconditions; patch granularity for nested maps; retry strategy on contention. | A: Update with `last_update_time` precondition + small retries (per spec)<br>B: Transaction-based lock (higher contention; risk of ABORTED 409) | Prototype / 1d | Build claim function stub + unit tests around conflict handling<br>Run contention simulation (two “invocations”) with forced precondition fail | Claim is atomic; losers no-op cleanly; deterministic step selection applied | — | Med |
| SPK-004 | P0 | error_retry_recovery | What is the recovery policy for steps stuck in `RUNNING` (crash after claim, before finalize)? | Without a lease/TTL, runs can deadlock forever; operators need a deterministic recovery path. | `docs/spec/error_and_retry_model.md#Partial failures (split-brain cases)`<br>`docs/spec/implementation_contract.md#Split-brain recovery: step is RUNNING but report artifact exists` | Lease TTL source of truth; who resets (orchestrator vs separate job); how to avoid resetting legitimate long runs. | A: Orchestrator resets `RUNNING` after TTL (simpler)<br>B: Dedicated “reaper” job/service (more ops; clearer separation) | Design / 4h | Define TTL fields/heuristics (based on `startedAt`)<br>Write state machine for `RUNNING → READY/FAILED` | Written policy + pseudo-tests; recovery is safe under duplicate events | SPK-003 | Low |
| SPK-005 | P0 | idempotency_concurrency | How will the worker avoid self-trigger loops / excessive invocations from its own Firestore patches? | Cost + noise; can amplify contention; can mask real issues. | `docs/spec/implementation_contract.md#Scenario A: No-op (no executable step)`<br>`docs/spec/system_integration.md#Delivery/ordering/idempotency guarantees` | Whether Eventarc/Firestore triggers include old/new fields; ability to filter by field paths; safe “noop” heuristics. | A: Rely on “no READY step → no-op” + deterministic selection (baseline)<br>B: Add extra guard (e.g., ignore events where only worker-owned fields changed) | Research / 4h | Verify trigger payload capabilities (what fields are available)<br>Prototype minimal “changed fields” filter if supported | Loop risk characterized; decision recorded; guard implemented (or explicitly rejected) | — | Low |
| SPK-006 | P0 | contracts_api | What is the canonical JSON Schema validation toolchain (draft level, canonicalization) and how is `schemaSha256` computed consistently? | Inconsistent validation or hashing causes drift bugs and false mismatches across envs/services. | `docs/contracts/llm_schema.md#Usage (MVP)`<br>`docs/contracts/llm_schema.schema.json` | JSON Schema draft support; canonical JSON encoding definition; where hash is computed (writer vs worker). | A: Compute hash when authoring schema docs; worker only logs/echoes<br>B: Worker computes canonical hash at runtime (more safety; needs strict canonicalization spec) | Design / 1d | Pick Python JSON Schema lib + draft<br>Define canonicalization algorithm (ordering, whitespace)<br>Write a hash test using `contracts/examples/llm_schema.example.json` | One documented hashing method; reproducible hash in tests; drift handling decided | — | Low |
| SPK-007 | P0 | integration_dependencies | Which Gemini endpoint is the production target (AI Studio API key vs Vertex AI with IAM/ADC), and what are the implications? | Affects auth, quotas, data residency/compliance, and ops model. | `docs/spec/system_integration.md#Google Gemini (LLM)`<br>`docs/spec/deploy_and_envs.md#Service account must have (minimum)` | Org security constraints; quota expectations; migration strategy; SDK differences (images + JSON schema support). | A: AI Studio (API key; faster MVP)<br>B: Vertex AI (IAM/ADC; stronger prod posture; more setup) | Research / 1d | Confirm org policy requirements<br>Prototype one request w/ structured output + images for chosen endpoint | Endpoint decision recorded; required IAM/secret model clarified | SPK-001 | Low |
| SPK-008 | P0 | performance_cost_quotas | How will the worker enforce the “finalize budget” and safely stop starting external calls when time is low? | Prevents timeouts that leave steps in bad states; reduces retries/cost. | `docs/spec/implementation_contract.md#Timeout policy (MVP)`<br>`docs/spec/deploy_and_envs.md#Timeouts (MVP recommendation)` | How to measure remaining time in CF gen2; handling long GCS downloads; cancellation semantics for Gemini calls. | A: Explicit time-budget utility + guard checks before each external call<br>B: Best-effort timeouts only (risk of running out of time mid-finalize) | Prototype / 1d | Implement `TimeBudgetPolicy` skeleton (per `static_model.md`)<br>Unit-test “remainingSeconds < finalizeBudget” paths | External calls are gated; finalize always attempted; behavior documented | SPK-003 | Med |
| SPK-009 | P0 | prompt_context_storage | What happens when context artifacts exceed size limits (64KB JSON / 256KB image) — fail fast vs degrade? | Unbounded context explodes token costs and can break provider requests; UX depends on chosen policy. | `docs/spec/prompt_storage_and_context.md#Context injection policy (MVP)`<br>`docs/spec/implementation_contract.md#Context artifact ingestion (policy)` | Whether “fail” is acceptable for prod runs; whether orchestrator can pre-check sizes; how to report actionable errors. | A: Fail with `INVALID_STEP_INPUTS` (spec default; deterministic)<br>B: Truncate/summarize inputs (non-deterministic; needs explicit contract) | Design / 4h | Confirm downstream expectations for failures<br>Define error message shape (sanitized) | Clear policy recorded; error behavior is testable and deterministic | — | Med |
| SPK-010 | P0 | integration_dependencies | Does the chosen Gemini SDK/endpoint support passing chart images as inline bytes (and with what limits/format)? | If image parts are unsupported, the core analysis quality may drop or the worker becomes non-functional. | `docs/spec/prompt_storage_and_context.md#Images (charts)` | Exact SDK API for multi-part content; MIME handling; per-request size caps; fallback policy. | A: Use SDK image parts (preferred; per spec)<br>B: No images; only textual chart descriptions (lower quality; may be unacceptable) | Prototype / 1d | Send a request with 1–3 PNGs + JSON response schema<br>Measure latency + failure modes | Verified API path; documented limits; integration decision made | SPK-007 | Low |
| SPK-011 | P1 | contracts_api | What is the “required subset” validation of `flow_runs/{runId}` and how is it maintained alongside strict JSON Schemas? | Overly strict validation breaks prototype reality; overly lax validation causes crashes/undefined behavior. | `docs/spec/implementation_contract.md#Validation policy (MVP)`<br>`docs/contracts/flow_run.md#Validation tolerance (MVP)` | Minimal required fields list; where validation failures map (run vs step); strategy for additionalProperties=false schemas. | A: Manual subset validation (explicit checks)<br>B: JSON Schema validation with relaxed options + allow unknown fields | Design / 1d | Enumerate required fields for worker paths<br>Create tests using `contracts/examples/flow_run.example.json` and negative variants | Subset contract documented; tests cover missing/wrong-type scenarios; error codes mapped | SPK-003 | Med |
| SPK-012 | P1 | data_model | How are deterministic artifact paths derived when `ARTIFACTS_PREFIX` is empty/changed and across envs? | Wrong naming breaks idempotency and makes artifacts hard to find/debug. | `docs/spec/system_integration.md#Cloud Storage artifacts`<br>`docs/spec/implementation_contract.md#Artifact naming (decision)` | Prefix normalization rules; bucket-per-env vs shared bucket; validating `timeframe` consistency with `stepId`. | A: Strict canonical builder (normalizes slashes; validates invariants)<br>B: Free-form concatenation (risk of drift and bad URIs) | Prototype / 4h | Implement `ArtifactPathPolicy` rules (per `static_model.md`)<br>Unit tests for edge cases (`/`, empty prefix, bad stepId) | Path builder stable + validated; invariants enforced; examples updated if needed | SPK-002 | High |
| SPK-013 | P1 | observability_slo | What is the minimal JSON logging implementation that guarantees required fields and prevents large/sensitive payload leakage? | Without strict logging, debugging and alerting become unreliable; data leakage risk rises. | `docs/spec/observability.md#Required fields (every log entry)` | JSON logger choice; how to inject trace/span IDs; log volume strategy (sampling). | A: Central `EventLogger` that enforces schema (per `static_model.md`)<br>B: Ad-hoc logging calls (drift likely) | Prototype / 4h | Implement a logger wrapper that rejects missing required keys<br>Add tests for redaction + payload sizing | Logging schema enforced; example logs match spec; no prompt/raw output logged | SPK-001 | Med |
| SPK-014 | P1 | error_retry_recovery | What retry/backoff implementation matches the time-budget constraints for Firestore/GCS/Gemini? | Unbounded retries can exceed timeout; too few retries cause flaky failures. | `docs/spec/error_and_retry_model.md#Retry / backoff / rate limiting`<br>`docs/spec/implementation_contract.md#Timeout policy (MVP)` | Attempt caps per dependency; what errors are retryable in chosen SDKs; jitter strategy. | A: `tenacity` with explicit stop/time limits<br>B: Custom minimal retry loop with time budget checks | Prototype / 4h | Map SDK exceptions → retryable flags<br>Write tests for “stop when remainingSeconds low” | Retry policy documented and implemented; no retries violate finalize budget | SPK-008 | Med |
| SPK-015 | P1 | testing_eval_quality | What is the test strategy that uses `test_vectors/**` to validate claim/no-op/finalize and structured output failure handling? | Prevents regressions in subtle idempotency and validation rules. | `docs/test_vectors/README.md`<br>`docs/fixtures/README.md` | How to simulate CloudEvent + Firestore reads; what to snapshot (patch output vs full state). | A: Pure unit tests with repository/LLM/GCS fakes using provided vectors<br>B: Integration tests against emulator/staging (more realistic; slower) | Design / 1d | Define harness inputs/outputs for handler<br>Write at least 3 golden tests: success, claim conflict, invalid structured output | Tests cover core paths; vectors are exercised; CI-ready approach chosen | SPK-003 | Med |
| SPK-016 | P1 | migration_versioning | How will prompt/schema updates be rolled out safely across running orchestrations (versioning and compatibility guarantees)? | Mismatched prompt/schema versions can break validation and cause failures at scale. | `docs/contracts/llm_schema.md#Immutability / versioning (MVP)`<br>`docs/spec/system_integration.md#Prompt / instruction storage (Firestore)` | Who sets `llmProfile.structuredOutput.schemaId`; compatibility rules across versions; rollback approach for prompt changes. | A: Orchestrator pins explicit versions in steps (spec-aligned)<br>B: Worker resolves “latest active” schema/prompt (simpler UX; risky, non-deterministic) | Design / 4h | Write rollout/rollback playbook (promptId/schemaId pinning)<br>Define compatibility contract for `metadata.schemaVersion` | Versioning policy documented; change procedure clear; avoids nondeterminism | SPK-011 | Med |
| SPK-017 | P2 | deployment_env_config | What is the deployment region/Eventarc routing strategy (and Firestore DB selection) to minimize latency and avoid cross-region pitfalls? | Cross-region triggers can increase latency/cost and introduce operational surprises. | `docs/spec/deploy_and_envs.md#Deployment model` | Firestore region vs function region; Eventarc routing; multi-env project layout. | A: Same region for Firestore, Eventarc, function (recommended)<br>B: Mixed regions (possible; higher latency/complexity) | Research / 4h | Confirm Firestore location<br>List supported regions for Firestore trigger + functions | Region decision recorded; deployment flags documented | — | Low |
| SPK-018 | P2 | security_privacy_compliance | What is the least-privilege IAM policy for the function’s service account across Firestore, GCS, and Secret Manager (and Vertex AI if chosen)? | Over-permissioned SA increases blast radius; under-permission breaks runtime. | `docs/spec/deploy_and_envs.md#Service account must have (minimum)` | Exact roles needed; per-env projects; resource-level IAM (bucket-level) vs project-level roles. | A: Minimal predefined roles + resource-level IAM bindings<br>B: Broad project roles (fast MVP; risky) | Design / 4h | Enumerate required API calls per dependency<br>Map to IAM roles and test with a staging deploy checklist | IAM list documented; principle of least privilege satisfied; includes optional Vertex path | SPK-007 | Low |
| SPK-019 | P2 | contracts_api | What is the canonical mapping from internal exceptions to stable `error.code`s in Firestore, and which failures are step-level vs run-level? | Consistent error codes enable orchestration policies and alerting; inconsistent mapping causes brittle automation. | `docs/spec/error_and_retry_model.md#Domain error codes (stable set proposal)`<br>`docs/spec/implementation_contract.md#Invariants` | Where to persist `FLOW_RUN_INVALID` (run vs step); handling missing `flow_run` vs invalid subject; message sanitization rules. | A: Central error mapper + enum (per `static_model.md#ErrorCode`)<br>B: Scatter mapping at call sites (drift likely) | Design / 4h | Create an error mapping table<br>Write tests that given failure type emits expected `error.code` and log event | Mapping is explicit and tested; orchestrator can rely on codes | SPK-013 | Med |
| SPK-020 | P2 | integration_dependencies | What is the “allowed models” policy per environment (allowlist enforcement, cost guardrails, and model deprecation handling)? | Prevents accidental expensive/unsafe models in prod and supports controlled rollouts. | `docs/spec/deploy_and_envs.md#Environment differences (minimum)`<br>`docs/spec/system_integration.md#Google Gemini (LLM)` | Where allowlist lives (env var vs Firestore); how to validate step-provided `llmProfile.modelName`; rollout for deprecations. | A: Env-configured allowlist enforced by worker<br>B: Orchestrator-only enforcement (worker trusts inputs; riskier) | Design / 4h | Define allowlist format<br>Add validation that rejects disallowed models with `LLM_PROFILE_INVALID` | Allowlist implemented and documented; rejection is testable | SPK-007 | Low |
